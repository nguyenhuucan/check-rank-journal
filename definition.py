import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import re
from datetime import datetime
from zoneinfo import ZoneInfo
from io import BytesIO
import webbrowser
from concurrent.futures import ThreadPoolExecutor, as_completed

# ========================
# Helper
# ========================

def clear_format(text):
    text = re.sub(r'\W+', ' ', text)
    return ' '.join(text.lower().split())

# ========================
# H√†m rank_h_q
# ========================

def check_rank_by_h_q(total_journals, percent, sjr_quartile):
        # X√°c ƒë·ªãnh rank_h d·ª±a tr√™n total_journals v√† Percent
        if total_journals >= 2000:
            thresholds = [0.5, 1, 5, 10, 18, 30, 43, 56, 69, 82]
        elif 1500 <= total_journals <= 1999:
            thresholds = [0.5, 2, 6, 11, 19, 31, 44, 57, 70, 83]
        elif 1000 <= total_journals <= 1499:
            thresholds = [0.5, 3, 7, 12, 20, 32, 45, 58, 71, 84]
        elif 500 <= total_journals <= 999:
            thresholds = [0.5, 4, 8, 13, 21, 33, 46, 59, 72, 85]
        elif 200 <= total_journals <= 499:
            thresholds = [0.9, 5, 10, 15, 23, 35, 48, 61, 74, 87]
        elif 50 <= total_journals <= 199:
            thresholds = [2.5, 6, 11, 16, 24, 36, 49, 62, 75, 88]
        elif 0 < total_journals < 50:
            thresholds = [3.5, 7, 15, 20, 28, 40, 53, 66, 79, 92]
        else:
            return 'None', 'None', 'L·ªói trong th·ªëng k√™ s·ªë l∆∞·ª£ng t·∫°p ch√≠'
        rank_h = next((i for i, th in enumerate(thresholds, start=0) if percent < th), 10)
        if rank_h < len(thresholds):
            Top_Percent = '< ' + str(thresholds[rank_h])
        else:
            Top_Percent = '>= ' + str(thresholds[-1])
        if (rank_h == 0) and (sjr_quartile == 'Q1'):
            return 'Ngo·∫°i h·∫°ng chuy√™n ng√†nh', Top_Percent, ''
        elif (rank_h == 1) and  (sjr_quartile == 'Q1'):
            return 'H·∫°ng 1', Top_Percent, ''
        elif (rank_h == 2) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2'):
            return 'H·∫°ng 2', Top_Percent, ''
        elif (rank_h == 3) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2'):
            return 'H·∫°ng 3', Top_Percent, ''
        elif (rank_h == 4) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2'):
            return 'H·∫°ng 4', Top_Percent, ''
        elif (rank_h == 5) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3'):
            return 'H·∫°ng 5', Top_Percent, ''
        elif (rank_h == 6) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3'):
            return 'H·∫°ng 6', Top_Percent, ''
        elif (rank_h == 7) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3'):
            return 'H·∫°ng 7', Top_Percent, ''
        elif (rank_h == 8) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3'):
            return 'H·∫°ng 8', Top_Percent, ''
        elif (rank_h == 9) and  (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3' or sjr_quartile == 'Q4'):
            return 'H·∫°ng 9', Top_Percent, ''
        elif (rank_h == 10) and (sjr_quartile == 'Q1' or sjr_quartile == 'Q2' or sjr_quartile == 'Q3' or sjr_quartile == 'Q4'):
            return 'H·∫°ng 10', Top_Percent, ''
        elif (0 <= rank_h <= 1) and (sjr_quartile == 'Q2'):
            return f'H·∫°ng 2', Top_Percent, f"R·ªõt t·ª´ H·∫°ng {rank_h} v√¨ Q2"
        elif (0 <= rank_h <= 4) and (sjr_quartile == 'Q3'):
            return f'H·∫°ng 5', Top_Percent, f"R·ªõt t·ª´ H·∫°ng {rank_h} v√¨ Q3"
        elif (0 <= rank_h <= 8) and (sjr_quartile == 'Q4'):
            return f'H·∫°ng 9', Top_Percent, f"R·ªõt t·ª´ H·∫°ng {rank_h} v√¨ Q4"
        else:
            return 'Kh√¥ng x·∫øp h·∫°ng', Top_Percent, 'Kh√¥ng c√≥ Q'

# ========================
# Crawler g·ªëc
# ========================

def find_title_or_issn(name_or_issn):
    url = f"https://www.scimagojr.com/journalsearch.php?q={name_or_issn}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    rows = []
    STT = 0
    for link in soup.find_all('a', href=True):
        if 'journalsearch.php?q=' in link['href']:
            title_journal = link.find('span', class_='jrnlname').text
            id_scopus_journal = link['href'].split('q=')[1].split('&')[0]
            detail_url = f"https://www.scimagojr.com/journalsearch.php?q={id_scopus_journal}&tip=sid&clean=0"
            detail_response = requests.get(detail_url)
            detail_soup = BeautifulSoup(detail_response.content, 'html.parser')

            issn = 'N/A'
            publisher = 'N/A'
            STT += 1

            pub_div = detail_soup.find('h2', string='Publisher')
            if pub_div:
                pub_p = pub_div.find_next('p')
                if pub_p:
                    publisher = pub_p.text.strip()

            issn_div = detail_soup.find('h2', string='ISSN')
            if issn_div:
                issn_p = issn_div.find_next('p')
                if issn_p:
                    issn = issn_p.text.strip()

            rows.append([STT, title_journal, issn, publisher, id_scopus_journal])

    return pd.DataFrame(rows, columns=['STT', 'T√™n t·∫°p ch√≠', 'ISSN', 'Nh√† xu·∫•t b·∫£n', 'ID Scopus'])

def id_scopus_to_all(id_scopus_input):
    url = f"https://www.scimagojr.com/journalsearch.php?q={id_scopus_input}&tip=sid&clean=0"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    name_journal = soup.find('h1').text.strip() if soup.find('h1') else 'N/A'
    country = soup.find('h2', string='Country').find_next('a').text.strip() if soup.find('h2', string='Country') else 'N/A'

    treecategory_dict = {}
    area = soup.find('h2', string='Subject Area and Category')
    if area:
        cats = area.find_next_sibling('p').find_all('li', recursive=True)
        for cat in cats:
            subcats = cat.find_all('li')
            if subcats:
                for sub in subcats:
                    name = sub.find('a').text.strip()
                    code = sub.find('a')['href'].split('=')[-1]
                    treecategory_dict[name] = code
            else:
                name = cat.find('a').text.strip()
                code = cat.find('a')['href'].split('=')[-1]
                treecategory_dict[name] = code

    publisher = soup.find('h2', string='Publisher').find_next('a').text.strip() if soup.find('h2', string='Publisher') else 'N/A'
    issn = soup.find('h2', string='ISSN').find_next('p').text.strip() if soup.find('h2', string='ISSN') else 'N/A'
    coverage = soup.find('h2', string='Coverage').find_next('p').text.strip() if soup.find('h2', string='Coverage') else 'N/A'
    homepage = soup.find('a', string='Homepage')['href'] if soup.find('a', string='Homepage') else 'N/A'
    howtopublish = soup.find('a', string='How to publish in this journal')['href'] if soup.find('a', string='How to publish in this journal') else 'N/A'
    email = soup.find('a', href=True, string=lambda x: x and '@' in x)
    email = email['href'].replace('mailto:', '') if email else 'N/A'

    return name_journal, country, treecategory_dict, publisher, issn, coverage, homepage, howtopublish, email

def check_rank_by_name_1_journal(search_name_journal, subject_area_category, year_check):
    rows = []
    STT = 0
    def fetch(url, category, id_cat, total, page):
        nonlocal STT
        r = requests.get(url)
        s = BeautifulSoup(r.content, 'html.parser')
        for row in s.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 4:
                name = cells[1].text.strip()
                if clear_format(name) == clear_format(search_name_journal):
                    STT += 1
                    pos = cells[0].text.strip()
                    q = cells[3].text.strip().split()[-1]
                    h = cells[4].text.strip()
                    percent = round(float(pos)/total*100, 5)
                    rank, top, note = check_rank_by_h_q(total, percent, q)
                    rows.append([STT, name, rank, q, int(h), int(pos), int(total), percent, top, category, id_cat, int(page), note])
                    break
    with ThreadPoolExecutor(max_workers=10) as ex:
        futures = []
        for cat, id_cat in subject_area_category.items():
            r = requests.get(f"https://www.scimagojr.com/journalrank.php?category={id_cat}&year={year_check}&type=j&order=h&ord=desc")
            s = BeautifulSoup(r.content, 'html.parser')
            total = int(s.find('div', class_='pagination').text.split()[-1]) if s.find('div', class_='pagination') else 0
            for page in range(1, int(total/20)+2):
                url = f"https://www.scimagojr.com/journalrank.php?category={id_cat}&year={year_check}&type=j&order=h&ord=desc&page={page}&total_size={total}"
                futures.append(ex.submit(fetch, url, cat, id_cat, total, page))
        for f in futures:
            f.result()
    return pd.DataFrame(rows, columns=['STT', 'T√™n t·∫°p ch√≠', 'H·∫°ng', 'Ch·ªâ s·ªë Q', 'H-index', 'V·ªã tr√≠', 'T·ªïng s·ªë t·∫°p ch√≠', 'Ph·∫ßn trƒÉm', 'Top ph·∫ßn trƒÉm', 'Chuy√™n ng√†nh', 'ID Chuy√™n ng√†nh', 'Trang', 'Ghi ch√∫'])

# === B·∫°n ƒë√£ c√≥ s·∫µn h√†m issn_to_all, ta gi·ªØ nguy√™n ===
def issn_to_all(issn):
    url = f"https://www.scimagojr.com/journalsearch.php?q={issn}&tip=sid&clean=0"
    response = requests.get(url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.content, 'html.parser')
    name_journal = soup.find('h1').text.strip() if soup.find('h1') else 'N/A'
    country_tag = soup.find('h2', string='Country')
    country = country_tag.find_next('a').text.strip() if country_tag else 'N/A'
    treecategory_dict = {}
    subject_area_div = soup.find('h2', string='Subject Area and Category')
    if subject_area_div:
        categories = subject_area_div.find_next_sibling('p').find_all('li', recursive=True)
        for category in categories:
            subcategories = category.find_all('li')
            if subcategories:
                for subcategory in subcategories:
                    subcategory_name = subcategory.find('a').text.strip()
                    subcategory_code = subcategory.find('a')['href'].split('=')[-1]
                    treecategory_dict[subcategory_name] = subcategory_code
            else:
                category_name = category.find('a').text.strip()
                category_code = category.find('a')['href'].split('=')[-1]
                treecategory_dict[category_name] = category_code
    subject_area_category = treecategory_dict
    publisher_tag = soup.find('h2', string='Publisher')
    publisher = publisher_tag.find_next('a').text.strip() if publisher_tag else 'N/A'
    h_index_tag = soup.find('h2', string='H-Index')
    h_index = h_index_tag.find_next('p', class_='hindexnumber').text.strip() if h_index_tag else 'N/A'
    issn_tag = soup.find('h2', string='ISSN')
    issn_info = issn_tag.find_next('p').text.strip() if issn_tag else 'N/A'
    coverage_tag = soup.find('h2', string='Coverage')
    coverage = coverage_tag.find_next('p').text.strip() if coverage_tag else 'N/A'
    homepage_tag = soup.find('a', string='Homepage')
    homepage_link = homepage_tag['href'] if homepage_tag else 'N/A'
    how_to_publish_tag = soup.find('a', string='How to publish in this journal')
    how_to_publish_link = how_to_publish_tag['href'] if how_to_publish_tag else 'N/A'
    email_tag = soup.find('a', href=True, string=lambda x: x and '@' in x)
    email_question_journal = email_tag['href'].replace('mailto:', '') if email_tag else 'N/A'
    return name_journal, country, subject_area_category, publisher, h_index, issn_info, coverage, homepage_link, how_to_publish_link, email_question_journal

# Check h·∫°ng 1 t·∫°p ch√≠    
def def_rank_by_name_or_issn(year):
    st.subheader(f"NƒÉm ƒëang tra c·ª©u ‚Äî {year}")
    st.markdown('<p style="color: gold; font-weight: bold; margin: 0;">B∆∞·ªõc 1</p>', unsafe_allow_html=True)
    keyword = st.text_input("Nh·∫≠p t√™n ho·∫∑c ISSN c·ªßa t·∫°p ch√≠. Sau ƒë√≥ b·∫•m **T√¨m ki·∫øm**")

    if st.button("T√¨m ki·∫øm"):
        with st.spinner("ƒêang t√¨m ..."):
            df = find_title_or_issn(keyword)
            st.session_state['df_search'] = df

    df = st.session_state.get('df_search', pd.DataFrame())
    st.info(f"ƒê√£ t√¨m th·∫•y {len(df)} k·∫øt qu·∫£, xem b·∫£ng b√™n d∆∞·ªõi")
    if not df.empty:
        st.dataframe(df, use_container_width=True, hide_index=True)
        st.markdown('<p style="color: gold; font-weight: bold; margin: 0;">B∆∞·ªõc 2</p>', unsafe_allow_html=True)
        options = df['STT'].astype(str) + " - " + df['T√™n t·∫°p ch√≠']
        choose = st.selectbox(
            "Ch·ªçn ƒë√∫ng t√™n t·∫°p ch√≠ mu·ªën tra c·ª©u. Sau ƒë√≥ b·∫•m **Xem h·∫°ng**",
            options,
            key="choose_journal"
        )

        if st.button("Xem h·∫°ng"):
            with st.spinner("ƒêang tra h·∫°ng ..."):
                choose_name = choose.split(' - ', 1)[1]
                selected = df[df['T√™n t·∫°p ch√≠'] == choose_name].iloc[0]
                id_scopus = selected['ID Scopus']
                issn = selected['ISSN']

                # Crawl chi ti·∫øt v√† b·∫£ng rank
                name_j, country, cats, pub, issn_detail, cover, home, howpub, mail = id_scopus_to_all(id_scopus)
                df_rank = check_rank_by_name_1_journal(name_j, cats, year)

                # L∆∞u v√†o session_state
                st.session_state['df_rank'] = df_rank
                st.session_state['id_scopus'] = id_scopus
                st.session_state['issn'] = issn
                st.session_state['publ'] = pub
                st.session_state['home'] = home

    df_rank = st.session_state.get('df_rank', pd.DataFrame())
    id_scopus = st.session_state.get('id_scopus')
    issn = st.session_state.get('issn')
    homepage_link_new = st.session_state.get('home')

    if not df_rank.empty and id_scopus and issn:
        st.dataframe(df_rank, use_container_width=True, hide_index=True)
        st.markdown('<p style="color: gold; font-weight: bold; margin: 0;">B∆∞·ªõc 3</p>', unsafe_allow_html=True)
        selected_line = st.selectbox(
            "Ch·ªçn chuy√™n ng√†nh ƒë·ªÉ xem chi ti·∫øt v√† m·ªü website SJR - Scopus - WoS in minh ch·ª©ng",
            df_rank['STT'].astype(str) + " - " + df_rank['T√™n t·∫°p ch√≠'] + " - " + df_rank['H·∫°ng'] + " - " + df_rank['Chuy√™n ng√†nh'],
            key="choose_line_rank"
        )

        if selected_line:
            stt_chosen = int(selected_line.split(' - ')[0])
            row_chosen = df_rank[df_rank['STT'] == stt_chosen].iloc[0]

            open_link_sjr = f"https://www.scimagojr.com/journalrank.php?category={row_chosen['ID Chuy√™n ng√†nh']}&year={year}&type=j&order=h&ord=desc&page={row_chosen['Trang']}&total_size={row_chosen['T·ªïng s·ªë t·∫°p ch√≠']}"
            open_link_scopus = f"https://www.scopus.com/sourceid/{id_scopus}"
            open_link_wos = f"https://mjl.clarivate.com:/search-results?issn={issn}&hide_exact_match_fl=true"

            now_vn = datetime.now(ZoneInfo("Asia/Ho_Chi_Minh")).strftime("%Hh%M ng√†y %d/%m/%Y") 
            publ = st.session_state.get('publ')
            if publ:
                st.info(
                    f"Th√¥ng tin tr√™n bi√™n b·∫£n nghi·ªám thu:\n"
                    f"+ T√™n t·∫°p ch√≠: {row_chosen['T√™n t·∫°p ch√≠']}\n"
                    f"+ ISSN: {issn}\n"
                    f"+ Nh√† xu·∫•t b·∫£n: {publ}\n"
                    f"+ {row_chosen['H·∫°ng']} (WoS), SJR: {row_chosen['Ch·ªâ s·ªë Q']}, H-Index: {row_chosen['H-index']}, thu·ªôc top {row_chosen['Top ph·∫ßn trƒÉm']}% (th·ª© t·ª± {row_chosen['V·ªã tr√≠']}) trong {row_chosen['T·ªïng s·ªë t·∫°p ch√≠']} t·∫°p ch√≠ v·ªÅ {row_chosen['Chuy√™n ng√†nh']}, truy xu·∫•t l√∫c {now_vn}"
                        )
            else:
                st.warning("NXB ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng b·∫•m 'Xem h·∫°ng' tr∆∞·ªõc.")
            
            st.markdown(
                f"""
                <a href="{open_link_sjr}">
                    \nüåê M·ªü website <span style="color: gold;">SJR</span> c·ªßa t·∫°p ch√≠
                    <span style="color: gold;">{row_chosen['T√™n t·∫°p ch√≠']}</span> ‚Äî
                    Chuy√™n ng√†nh <span style="color: gold;">{row_chosen['Chuy√™n ng√†nh']}</span>  ‚Äî
                    ISSN: <span style="color: gold;">{issn}</span>
                </a>
                """,
                unsafe_allow_html=True
            )

            st.markdown(
                f"""
                <a href="{open_link_scopus}">
                    \nüåê M·ªü website <span style="color: gold;">Scopus</span> c·ªßa t·∫°p ch√≠
                    <span style="color: gold;">{row_chosen['T√™n t·∫°p ch√≠']}</span>
                </a>
                """,
                unsafe_allow_html=True
            )

            st.markdown(
                f"""
                <a href="{open_link_wos}">
                    \nüåê M·ªü website <span style="color: gold;">MJL-WoS</span> c·ªßa t·∫°p ch√≠
                    <span style="color: gold;">{row_chosen['T√™n t·∫°p ch√≠']}</span>
                </a>
                """,
                unsafe_allow_html=True
            )

            st.markdown(
                f"""
                <a href="{homepage_link_new}">
                    \nüåê M·ªü website <span style="color: gold;">ch√≠nh th·ª©c</span> c·ªßa t·∫°p ch√≠ <span style="color: gold;">{row_chosen['T√™n t·∫°p ch√≠']}</span>
                </a>
                """,
                unsafe_allow_html=True
            )
            issn_first = issn.split(',')[0].strip()
            link_issn_portal = f"https://portal.issn.org/resource/ISSN/{issn_first}"
            st.markdown(
                f"""
                <a href="{link_issn_portal}" target="_blank">
                    üåê M·ªü Website <span style="color: gold;">ISSN Portal</span> c·ªßa t·∫°p ch√≠
                    <span style="color: gold;">{row_chosen['T√™n t·∫°p ch√≠']}</span> (ki·ªÉm tra ngu·ªìn g·ªëc t·∫°p ch√≠ tr√™n h·ªá th·ªëng portal.issn.org)
                </a>
                """,
                unsafe_allow_html=True
            )

def check_rank_by_name_1_category(id_category, year_check):
    row_add = []
    STT = 0
    url_category = f"https://www.scimagojr.com/journalrank.php?category={id_category}&type=j&order=h&ord=desc&year={year_check}"
    response = requests.get(url_category)
    soup = BeautifulSoup(response.content, 'html.parser')
    pagination_div = soup.find('div', class_='pagination')
    total_journals_text = pagination_div.text.strip() if pagination_div else '0'
    total_journal = int(total_journals_text.split()[-1]) if total_journals_text.split() else 0

    for page_number in range(1, int(total_journal / 20) + 2):
        url = f"https://www.scimagojr.com/journalrank.php?category={id_category}&year={year_check}&type=j&order=h&ord=desc&page={page_number}&total_size={total_journal}"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        rows = soup.find_all('tr')
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 4:
                name_journal_web = cells[1].text.strip()
                STT += 1
                position = cells[0].text.strip()
                SJR_Q_value = cells[3].text.strip()
                if ' ' in SJR_Q_value:
                    _, Q_value = SJR_Q_value.split()
                else:
                    Q_value = 'N/A'
                h_index = cells[4].text.strip()
                percent = round((float(position) / total_journal * 100), 5) if total_journal else 0
                rank, top_percent, note = check_rank_by_h_q(total_journal, percent, Q_value)
                row_add.append([STT, name_journal_web, rank, Q_value, int(h_index), int(position), total_journal, percent, top_percent, page_number, note])

    df = pd.DataFrame(row_add, columns=[
        'STT', 'T√™n t·∫°p ch√≠', 'H·∫°ng', 'Ch·ªâ s·ªë Q', 'H-index', 'V·ªã tr√≠',
        'T·ªïng s·ªë t·∫°p ch√≠', 'Ph·∫ßn trƒÉm', 'Top ph·∫ßn trƒÉm', 'Trang', 'Ghi ch√∫'
    ])
    return df

def def_list_all_subject(year):
    st.subheader(f"NƒÉm ƒëang tra c·ª©u ‚Äî {year}")

    if st.button("üì•T·∫£i danh s√°ch t·∫•t c·∫£ c√°c chuy√™n ng√†nh"):
        with st.spinner("ƒêang t·∫£i danh s√°ch..."):
            url = f"https://www.scimagojr.com/journalrank.php?year={year}"
            response = requests.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')

            all_subject_li = None
            for li in soup.find_all('li'):
                a_tag = li.find('a')
                if a_tag and 'All subject categories' in a_tag.get_text():
                    all_subject_li = li
                    break

            rows = []
            if all_subject_li:
                list_items = all_subject_li.find_next_siblings('li')
                for item in list_items:
                    a_tag = item.find('a')
                    if a_tag:
                        name = a_tag.get_text(strip=True)
                        code = a_tag['href'].split('=')[-1]
                        rows.append([name, code])

            if rows:
                df = pd.DataFrame(rows, columns=['T√™n chuy√™n ng√†nh', 'ID Chuy√™n ng√†nh'])
                df.insert(0, 'STT', range(1, len(df) + 1))
                st.session_state['df_subject_list'] = df
                st.success(f"‚úÖ ƒê√£ t·∫£i {len(df)} chuy√™n ng√†nh")
            else:
                st.warning("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu")

    df = st.session_state.get('df_subject_list', pd.DataFrame())

    if not df.empty:
        st.dataframe(df, use_container_width=True, hide_index=True)

        options = df['STT'].astype(str) + " - " + df['T√™n chuy√™n ng√†nh'] + " - " + df['ID Chuy√™n ng√†nh']
        choose = st.selectbox(
            "Ch·ªçn chuy√™n ng√†nh ƒë·ªÉ xem th√¥ng tin",
            options
        )

        if 'chosen_category' not in st.session_state:
            st.session_state['chosen_category'] = {}

        if st.button("‚úÖ X√°c nh·∫≠n v√† t·∫°o link SJR"):
            stt, name, code = choose.split(' - ')
            st.session_state['chosen_category']['name'] = name
            st.session_state['chosen_category']['code'] = code

            link_sjr = f"https://www.scimagojr.com/journalrank.php?category={code}&year={year}&type=j&order=h&ord=desc"
            st.success(f"ƒê√£ ch·ªçn chuy√™n ng√†nh: **{name}**")
            st.markdown(f"[üåê M·ªü website SJR chuy√™n ng√†nh **{name}**]({link_sjr})")

        if 'chosen_category' in st.session_state and st.session_state['chosen_category']:
            name = st.session_state['chosen_category']['name']
            code = st.session_state['chosen_category']['code']
            if st.button(f"üîç Xem h·∫°ng t·∫•t c·∫£ c√°c t·∫°p ch√≠ thu·ªôc chuy√™n ng√†nh **{name}**"):
                with st.spinner(f"ƒêang x·∫øp h·∫°ng t·∫•t c·∫£ c√°c t·∫°p ch√≠ thu·ªôc chuy√™n ng√†nh **{name}** ..."):
                    df_rank = check_rank_by_name_1_category(code, year)
                    if not df_rank.empty:
                        st.dataframe(df_rank, use_container_width=True, hide_index=True)

                        buffer = BytesIO()
                        df_rank.to_excel(buffer, index=False)
                        buffer.seek(0)

                        st.download_button(
                            label=f"üì• T·∫£i file excel x·∫øp h·∫°ng c·ªßa chuy√™n ng√†nh **{name}**",
                            data=buffer,
                            file_name=f"rank_{name}_{year}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    else:
                        st.warning("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu x·∫øp h·∫°ng")
    else:
        st.info("üëâ B·∫•m **T·∫£i danh s√°ch chuy√™n ng√†nh** ƒë·ªÉ b·∫Øt ƒë·∫ßu")


def def_check_in_scopus_sjr_wos(year):
    st.subheader(f"Ph√¢n lo·∫°i t·∫°p ch√≠ thu·ªôc SJR - SCOPUS - WOS")

    keyword = st.text_input("Nh·∫≠p t√™n ho·∫∑c ISSN c·ªßa t·∫°p ch√≠")

    search_clicked = st.button("üîç T√¨m ki·∫øm")

    if search_clicked:
        st.session_state['keyword_journal'] = keyword.strip()
        st.session_state['search_done'] = False

    if 'keyword_journal' in st.session_state and not st.session_state.get('search_done', False):
        try:
            with st.spinner(f"ƒêang t√¨m t·∫°p ch√≠ theo '{st.session_state['keyword_journal']}'..."):
                df_1 = find_title_or_issn(st.session_state['keyword_journal'])
                empty_indices = df_1[df_1['ISSN'].str.len() < 4].index.tolist()
                df_1.drop(index=empty_indices, inplace=True)
                st.session_state['df_journals'] = df_1
                st.session_state['search_done'] = True
        except Exception as e:
            st.warning(f"‚ö†Ô∏è C·∫£nh b√°o l·ªói >>> {str(e)}")

    if st.session_state.get('search_done', False):
        df_1 = st.session_state['df_journals']

        column_show = ['STT', 'T√™n t·∫°p ch√≠', 'ISSN', 'Nh√† xu·∫•t b·∫£n', 'ID Scopus']
        st.dataframe(df_1[column_show], use_container_width=True, hide_index=True)

        options = [f"{a} - {b} - {c} - {d}"
                   for a, b, c, d in zip(df_1['STT'], df_1['T√™n t·∫°p ch√≠'],
                                         df_1['ISSN'], df_1['Nh√† xu·∫•t b·∫£n'])]

        selected_option = st.selectbox(
            f"ƒê√£ t√¨m th·∫•y {len(df_1)} k·∫øt qu·∫£ - Ch·ªçn t·∫°p ch√≠ mu·ªën tra c·ª©u",
            options,
            key="selected_journal"
        )

        choose_stt = selected_option.split(' - ')[0]
        selected_row = df_1.iloc[int(choose_stt) - 1]
        id_scopus_choose = selected_row['ID Scopus']

        # N√∫t Tra c·ª©u
        tra_cuu_clicked = st.button("üìë Tra c·ª©u th√¥ng tin chi ti·∫øt")

        if tra_cuu_clicked:
            try:
                with st.spinner("üîÑ ƒêang tra c·ª©u th√¥ng tin chi ti·∫øt..."):
                    (
                        name_journal_check,
                        country,
                        subject_area_category_check,
                        publisher,
                        h_index,
                        issn_check,
                        coverage,
                        homepage_link,
                        how_to_publish_link,
                        email_question_journal
                    ) = issn_to_all(id_scopus_choose)

                    st.session_state['journal_detail'] = {
                        "name_journal_check": name_journal_check,
                        "country": country,
                        "subject_area_category_check": subject_area_category_check,
                        "publisher": publisher,
                        "h_index": h_index,
                        "issn_check": issn_check,
                        "coverage": coverage,
                        "homepage_link": homepage_link,
                        "how_to_publish_link": how_to_publish_link,
                        "email_question_journal": email_question_journal
                    }
            except Exception as e:
                st.warning(f"‚ö†Ô∏è C·∫£nh b√°o l·ªói khi tra c·ª©u >>> {str(e)}")

        # Lu√¥n hi·ªÉn th·ªã n·∫øu ƒë√£ tra c·ª©u
        if 'journal_detail' in st.session_state:
            detail = st.session_state['journal_detail']
            issn_check = detail["issn_check"]
            name_journal_check = detail["name_journal_check"]
            subject_area_category_check = detail["subject_area_category_check"]
            homepage_link = detail['homepage_link']

            open_link_sjr = f"https://www.scimagojr.com/journalsearch.php?q={id_scopus_choose}&tip=sid&clean=0"
            open_link_scopus = f"https://www.scopus.com/sourceid/{id_scopus_choose}"
            open_link_wos = f"https://mjl.clarivate.com/search-results?issn={issn_check}&hide_exact_match_fl=true&utm_source=mjl&utm_medium=share-by-link&utm_campaign=search-results-share-this-journal"

            st.markdown(
                f"""B·∫°n ƒëang xem th√¥ng tin c·ªßa t·∫°p ch√≠ <span style="color: gold;">{name_journal_check}</span> v·ªõi m√£ s·ªë ISSN l√† <span style="color: gold;">{issn_check}</span>""",
                unsafe_allow_html=True
            )

            col1, col2 = st.columns(2)
            with col1:
                st.markdown(
                    f"""
                    <a href="{open_link_sjr}" target="_blank">
                        \nüåê M·ªü website <span style="color: gold;">SJR</span> c·ªßa t·∫°p ch√≠ ƒëang xem
                    </a>
                    """,
                    unsafe_allow_html=True
                )
                st.markdown(
                    f"""
                    <a href="{open_link_scopus}" target="_blank">
                        \nüåê M·ªü website <span style="color: gold;">Scopus</span> c·ªßa t·∫°p ch√≠ ƒëang xem
                    </a>
                    """,
                    unsafe_allow_html=True
                )
                st.markdown(
                    f"""
                    <a href="{open_link_wos}" target="_blank">
                        \nüåê M·ªü website <span style="color: gold;">MJL-WoS</span> c·ªßa t·∫°p ch√≠ ƒëang xem
                    </a>
                    """,
                    unsafe_allow_html=True
                )
                st.markdown(
                    f"""
                    <a href="{homepage_link}" target="_blank">
                        \nüåê M·ªü website <span style="color: gold;">HomePage</span> c·ªßa t·∫°p ch√≠ ƒëang xem
                    </a>
                    """,
                    unsafe_allow_html=True
                )

            with col2:
                st.markdown(f""" ‚úÖ Qu·ªëc gia: <span style="color: gold;">{detail['country']}</span> """, unsafe_allow_html=True)
                st.markdown(f""" ‚úÖ Nh√† xu·∫•t b·∫£n: <span style="color: gold;">{detail['publisher']}</span> """, unsafe_allow_html=True)
                st.markdown(f""" ‚úÖ H-index: <span style="color: gold;">{detail['h_index']}</span> """, unsafe_allow_html=True)

                df_subjects = pd.DataFrame(
                    [(i + 1, name, id) for i, (name, id) in enumerate(subject_area_category_check.items())],
                    columns=["STT", "Chuy√™n ng√†nh h·∫πp", "ID chuy√™n ng√†nh h·∫πp"]
                )
                st.markdown(f""" ‚úÖ S·ªë chuy√™n ng√†nh h·∫πp: <span style="color: gold;">{len(df_subjects)}</span>""", unsafe_allow_html=True)

            st.markdown(
                f"""Chi ti·∫øt <span style="color: gold;">{len(df_subjects)}</span> chuy√™n ng√†nh h·∫πp c·ªßa t·∫°p ch√≠ <span style="color: gold;">{name_journal_check}</span>""",
                unsafe_allow_html=True
            )
            st.dataframe(df_subjects, use_container_width=True, hide_index=True)



# Tab 4 -----------------------

def def_rank_by_rank_key(year):
    st.subheader(f"L·ªçc t·∫°p ch√≠ theo T·ª´ kho√° chuy√™n ng√†nh v√† H·∫°ng ‚Äî {year}")

    # ‚úÖ Lu√¥n c√≥ s·∫µn, kh√¥ng b·ªã m·∫•t khi rerun
    column_show = [
        'STT', 'T√™n t·∫°p ch√≠', 'H·∫°ng', 'Ch·ªâ s·ªë Q', 'H-index',
        'V·ªã tr√≠', 'T·ªïng s·ªë t·∫°p ch√≠', 'Top ph·∫ßn trƒÉm',
        'Chuy√™n ng√†nh', 'Ghi ch√∫'
    ]

    # B∆∞·ªõc 1: Ch·ªçn t·ª´ kho√° + nh·∫≠p th√™m
    list_keywords = [
        "math", "analysis", "numerical", "optimization", "control",
        "algebra", "geometry", "statistics", "probability",
        "computer", "computation", "engineering"
    ]
    selected_keywords = st.multiselect(
        "üîë Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu ho·∫∑c ghi th√™m t·ª´ kho√°",
        options=list_keywords
    )
    new_keyword = st.text_input("Ho·∫∑c nh·∫≠p th√™m c√°c t·ª´ kho√° kh√°c (c√°c t·ª´ kh√≥a c√°ch nhau d·∫•u ph·∫©y)")

    # B∆∞·ªõc 2: Ch·ªçn H·∫°ng
    rank_options = ["Ngo·∫°i h·∫°ng chuy√™n ng√†nh"] + [f"H·∫°ng {i}" for i in range(1, 11)]
    selected_ranks = st.multiselect(
        "üè∑Ô∏è Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu H·∫°ng ƒë·ªÉ l·ªçc",
        options=rank_options
    )

    if st.button("üîç L·ªçc t·∫°p ch√≠"):
        try:
            # Gom t·ª´ kho√°
            all_keywords = selected_keywords.copy()
            if new_keyword.strip():
                all_keywords.extend([k.strip() for k in new_keyword.split(',') if k.strip()])
            all_keywords = list(set(all_keywords))  # Xo√° tr√πng

            #st.write(f"üîë T·ª´ kho√°: {all_keywords}")
            #st.write(f"üè∑Ô∏è H·∫°ng: {selected_ranks}")

            if not all_keywords or not selected_ranks:
                st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 t·ª´ kho√° v√† 1 H·∫°ng")
                return

            # Gi·ªØ ch·ªó hi·ªÉn th·ªã tr·∫°ng th√°i & b·∫£ng
            status_placeholder = st.empty()
            dataframe_placeholder = st.empty()

            result_df = pd.DataFrame()

            # L·∫•y danh s√°ch ng√†nh
            with st.spinner("ƒêang l·ªçc d·ªØ li·ªáu ..."):
                url = "https://www.scimagojr.com/journalrank.php?type=j&order=h&ord=desc"
                response = requests.get(url)
                soup = BeautifulSoup(response.content, 'html.parser')

                all_subject_categories = None
                for li in soup.find_all('li'):
                    a_tag = li.find('a')
                    if a_tag and 'All subject categories' in a_tag.get_text():
                        all_subject_categories = li
                        break

                list_items = all_subject_categories.find_next_siblings('li')
                categories = []
                for item in list_items:
                    a_tag = item.find('a')
                    if a_tag:
                        category = item.get_text()
                        category_id = a_tag['href'].split('=')[-1]
                        categories.append((category, category_id))

            for category, category_id in categories:
                for name_key in all_keywords:
                    if clear_format(name_key) in clear_format(category):
                        status_placeholder.info(f"ƒêang ki·ªÉm tra chuy√™n ng√†nh: **{category}**")
                        with st.spinner("ƒêang l·ªçc d·ªØ li·ªáu ..."):
                            df_category = check_rank_by_name_1_category(category_id, year)
                            for rank in selected_ranks:
                                filtered_df = df_category[df_category['H·∫°ng'] == rank].copy()
                                if not filtered_df.empty:
                                    filtered_df.insert(filtered_df.columns.get_loc('Top ph·∫ßn trƒÉm') + 1, 'Chuy√™n ng√†nh', category)
                                    filtered_df.insert(filtered_df.columns.get_loc('Chuy√™n ng√†nh') + 1, 'ID Chuy√™n ng√†nh', category_id)
                                    result_df = pd.concat([result_df, filtered_df], ignore_index=True)
                                    result_df['STT'] = range(1, len(result_df) + 1)
                                    dataframe_placeholder.dataframe(result_df[column_show], hide_index=True)

            # L∆∞u l·∫°i ƒë·ªÉ n√∫t t·∫£i kh√¥ng m·∫•t khi rerun
            st.session_state['result_df'] = result_df

        except Exception as e:
            st.error(f"üö´ >>> C·∫£nh b√°o l·ªói >>> {str(e)}")

    # ===========================
    # Ch·ªâ hi·ªán n√∫t t·∫£i, KH√îNG render b·∫£ng l·∫ßn 2
    # ===========================
    if 'result_df' in st.session_state and not st.session_state['result_df'].empty:
        result_df = st.session_state['result_df']

        towrite = BytesIO()
        result_df[column_show].to_excel(towrite, index=False, engine='xlsxwriter')
        towrite.seek(0)

        st.download_button(
            label="üíæ T·∫£i file excel k·∫øt qu·∫£ l·ªçc",
            data=towrite,
            file_name=f"Result_Rank_Keyword_{year}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )


def def_rank_by_Q_key(year):
    st.subheader(f"üîé L·ªçc t·∫°p ch√≠ theo T·ª´ kho√° chuy√™n ng√†nh v√† Ch·ªâ s·ªë Q ‚Äî {year}")

    # ‚úÖ LU√îN C√ì: D√ô B·∫§M T·∫¢I FILE TH√å V·∫™N C√íN
    column_show = [
        'STT', 'T√™n t·∫°p ch√≠', 'H·∫°ng', 'Ch·ªâ s·ªë Q', 'H-index',
        'V·ªã tr√≠', 'T·ªïng s·ªë t·∫°p ch√≠', 'Top ph·∫ßn trƒÉm',
        'Chuy√™n ng√†nh', 'Ghi ch√∫'
    ]

    # B∆∞·ªõc 1: T·ª´ kho√°
    list_keywords = [
        "math", "analysis", "numerical", "optimization", "control",
        "algebra", "geometry", "statistics", "probability",
        "computer", "computation", "engineering"
    ]
    selected_keywords = st.multiselect(
        "üîë Ch·ªçn t·ª´ kho√° g·ª£i √Ω (c√≥ th·ªÉ ch·ªçn nhi·ªÅu):",
        options=list_keywords
    )
    new_keyword = st.text_input("Ho·∫∑c nh·∫≠p th√™m c√°c t·ª´ kho√° kh√°c (c√°ch nhau d·∫•u ph·∫©y)")

    # B∆∞·ªõc 2: Ch·ªçn ch·ªâ s·ªë Q
    Q_options = [f"Q{i}" for i in range(1, 5)]
    selected_Qs = st.multiselect(
        "üè∑Ô∏è Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu ch·ªâ s·ªë Q:",
        options=Q_options
    )

    # ‚úÖ KEY RI√äNG ƒë·ªÉ tr√°nh Duplicate ID
    if st.button("üîç L·ªçc t·∫°p ch√≠", key="button_q_key"):
        try:
            all_keywords = selected_keywords.copy()
            if new_keyword.strip():
                all_keywords.extend([k.strip() for k in new_keyword.split(',') if k.strip()])
            all_keywords = list(set(all_keywords))

            if not all_keywords or not selected_Qs:
                st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 t·ª´ kho√° v√† 1 ch·ªâ s·ªë Q")
                return

            # Placeholder
            status_placeholder = st.empty()
            dataframe_placeholder = st.empty()

            result_df = pd.DataFrame()

            with st.spinner("ƒêang t·∫£i danh s√°ch chuy√™n ng√†nh ..."):
                url = "https://www.scimagojr.com/journalrank.php?type=j&order=h&ord=desc"
                response = requests.get(url)
                soup = BeautifulSoup(response.content, 'html.parser')

                all_subject_categories = None
                for li in soup.find_all('li'):
                    a_tag = li.find('a')
                    if a_tag and 'All subject categories' in a_tag.get_text():
                        all_subject_categories = li
                        break

                list_items = all_subject_categories.find_next_siblings('li')
                categories = []
                for item in list_items:
                    a_tag = item.find('a')
                    if a_tag:
                        category = item.get_text()
                        category_id = a_tag['href'].split('=')[-1]
                        categories.append((category, category_id))

            for category, category_id in categories:
                for name_key in all_keywords:
                    if clear_format(name_key) in clear_format(category):
                        status_placeholder.info(f"ƒêang ki·ªÉm tra chuy√™n ng√†nh: **{category}**")
                        with st.spinner("ƒêang l·ªçc d·ªØ li·ªáu ..."):
                            df_category = check_rank_by_name_1_category(category_id, year)
                            for q in selected_Qs:
                                filtered_df = df_category[df_category['Ch·ªâ s·ªë Q'] == q].copy()
                                if not filtered_df.empty:
                                    filtered_df.insert(filtered_df.columns.get_loc('Top ph·∫ßn trƒÉm') + 1, 'Chuy√™n ng√†nh', category)
                                    filtered_df.insert(filtered_df.columns.get_loc('Chuy√™n ng√†nh') + 1, 'ID Chuy√™n ng√†nh', category_id)
                                    result_df = pd.concat([result_df, filtered_df], ignore_index=True)
                                    result_df['STT'] = range(1, len(result_df) + 1)
                                    dataframe_placeholder.dataframe(result_df[column_show], hide_index=True)

            # L∆∞u l·∫°i, kh√¥ng m·∫•t khi b·∫•m T·∫£i
            st.session_state['result_df_q'] = result_df

        except Exception as e:
            st.error(f"üö´ >>> C·∫£nh b√°o l·ªói >>> {str(e)}")

    # ‚úÖ SAU KHI C√ì K·∫æT QU·∫¢: CH·ªà HI·ªÜN N√öT T·∫¢I, KH√îNG RENDER L·∫†I B·∫¢NG
    if 'result_df_q' in st.session_state and not st.session_state['result_df_q'].empty:
        result_df = st.session_state['result_df_q']

        towrite = BytesIO()
        result_df[column_show].to_excel(towrite, index=False, engine='xlsxwriter')
        towrite.seek(0)

        st.download_button(
            label="üíæ T·∫£i file Excel k·∫øt qu·∫£ l·ªçc",
            data=towrite,
            file_name=f"Result_Q_Keyword_{year}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
